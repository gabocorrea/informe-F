ids ya chekeados manualmente: 1-2005 y 7324-7737
last:
	acabo de lograr que el output de la web se corra desde exec.php... y crea otro archivo con el export de la web correcto.
	..ver que falta hacer..
	falta pasar eso a un csv con header text,_class_  .. luego pasarlo a .arff


El informe debe ser escrito en tiempo presente, usando tercera persona
singular
- ver como escribio el informe el FF


todo's:
	analizar mas archivos de la carpeta out
	add min num lines in comment to consider it .. to 1_...py script   (que sea 4 o mas lineas)
	esquema Informe
*	fix bug escribir id en index.html
*	fix problema <li>,<ol>,<ul>,</li>, ...etc al separar por frases usando el script.
*	linea 554:	tiene mal extraido el texto del comentario... está incompleto ...-> entonces -> dataset.xml no fue correctamente convertido a .csv    ...      linea 11980 ? quizas tambien le pasa lo mismo.... porque tambien su primer valor era un comentario en las comas.
*	(opcional)	fix bug,que los hotkeys no funcionen al estar escribiendo.
	probar con un param que indique el numero de directivas de una frase y pasarle eso a weka
	(GUI)		en index.html hacer que la URL esté con left-justification (actualmente está en el centro y se ve mal). además hacer que no se vea  el #1 o #°n  al final de la URL
	
caroo! un abrazote y bessos para ti en tu cumpleaños!! que lo pases bien!






_________________________________
PARTES PENDIENTES DEL DESARROLLO
----
1)	Extraer comentarios
	1.1)*	extraer todos los comentarios
	1.2)*	(asignar cada comentario a su archivo,clase,posicion dentro de archivo, etc)
	1.3)	separar por frases
	1.4)	crear .csv (pues lo usa index.html)
2)	Revision Manual en pagina web
	2.?)*	Elección de tipos o clases de comentarios (ej. directive, non-directive, null-directive)
3)	Exportar resultados
	3.1)	Juntar output .csv de index.html con el .csv original usado por la webpage
	3.2)	Crear .arff a partir del .csv final (util para utilizar ML despues)
	...





ids issues:
	8374	-	 Frase aparece varias veces (mas de 15) Constructor that performs no validation. Use getInstance if you want that.			algo de overfitting!











IDs:
Eclipse JFace: 		87 - 7319
Apache Commons:   	7324 - 10404
Java:				10405 - 20575    



tokenizer01:
tokenizer02: 
 	.,;:'"()?!#<>$&{}+

stopwords! ::::: quizas sea necesario quitarle al archivo de SWords las palabras del select_de_weka (archivo attr_select_result_01)

probar con 10 atributos
leer sobre string to word vector filter
svm lineal con regularización?? bajar liblinear
usar un test de 500 instancias.  o   2 tests 
tokenizer, delimiters

	






mejora:
	la regex \binvo.*\b toma incorrectamente la frase 'org.apache.commons.collections.functors.InvokerTransformer'. mejorar la regex

IDEA IMPORTANTE:
	quizas seria buena idea detectar directivas con keywords, dependiendo del contexto. Si estoy en un tag @param y encuentro null, es directiva (probablemente) porque probablemente diga '...; must not be null'. Si estoy en un comentario de metodo, quizas sea menos probable que es directiva. ... etc (debo buscar mas ejemplos para esta idea aun, tengo poca evidencia)

ideas:
	la correccion manual de directives es lenta y tediosa. seria mejor correr el programa, que haga highlight a todos los directives segun algun criterio. si luego se ve que hay exceso de highlights, se aumenta el thershold (o algun parametro) que disminuya la cantidad de highlights.

notas del 1er archivo analizado:
	id 207 - 'backwards compatibility'
	id 232 - 'static' y 'prevent' no son keywords.. raro quizas que no lo sean
	*blocking* podria ser keyword?  es parecido a lock*
	deprecated maybe should be a keyword
	bad practive maybe should be a keyword
	null maybe should be a keyword
	until maybe should/could be a (weak) keyword
	disables maybe should/could be a (weak) keyword
	if (should it be a weak keyword??)
	static maybe should be a keyword
	error maybe should NOT be a keyword (sometimes it confuses results.. makes normal comments that talk about error objects and others to be tagged as directives)
	warning maybe should NOT be ...  "    "
	why is 'don't' not a keyword?
	'no effect' o 'has no effect' podria ser keyword
	'does nothing' podria ser keyword
	'usually' ? quizas
	keyword after quizas debe ser after*
			 ejemplo: Afterward, the methods load() and save() can be used to load and store this preference store.
	workaround
			ejemplo: Workaround for https://bugs.eclipse.org/bugs/show_bug.cgi?id=23980 : Shells without borders are not resizable on GTK.
	'immutable' podria ser keyword
	

	should   podria no ser una directiva. al parecer hay varias frases no directivas que si tienen la palabra should

	why 'Return Value Directive' (from monperrus .pdf) makes a directive from cases when we have 'returns A or null'   ,.. isn't it the same as 'resturns true or false' ?







en el informe hablar de:
	PREP DE DATOS:
		Hablar de la conversion hecha de .xml a .csv  Herramientas existentes no funcionaron, se tuvo que hacer a la medida ("a mano") para los datos. (necesaria para el posterior analisis usando ML en Weka)

		seleccion de las no-directivas, necesarias para la etapa de text mining. asi tener mismo numero de directivas, que de no-directivas, y entrenar a un algoritmo de machine learning con esos datos. Esto consistio en sacar frases de el dataset de Monperrus, que no estuvieran en el archivo con las directivas, y además, que no tuvieran ningun keyword (explicar uso de regexp). Aun asi, es posible que un comentario sea directiva y no tenga ningun keyword de Monperrus-->un chequeo manual de las frases fue necesario. Los datos eran muchos. Ademas venian en formato complicado... por ejemplo, sin IDs --> correlacionar directivas con dataset se complicó... habian muchos caracteres como tags, comillas..etc por la naturaleza de los comentarios de codigo.

		Se notó que era necesario separar los comentarios por sus frases. porque las frases son destacadas como directivas (no todo el comentario). Explicar uso de regex no trivial.

		Luego de un tiempo corrigiendo manualmente los comentarios, se notó que tomaba demasiado tiempo y solo una libreria estaba siendo corregida. Para mejorar los datos usados por Weka, se obtuvo una muestra randomizada (explicar xq randomizada; overfitting) de 500 comentarios de cada una de las 3 librerias, solo de metodos (decir xq solo metodos)... y solo comentarios con 3 o mas frases (los con 2 o 1 frase suelen ser no-directivas y son casos de menor interes)

		en el dataset (todos los datos de monperrus) se anotaron como directivas todas las frases que tenian al menos 1 keyword (y ninguna frase de las directivas originales de Monperrus) y el resto como no-directiva. esto permite ver como destacadas de forma automatica en la pagina web las frases con keywords de monperrus (que suelen ser de verdad directivas)

	WEB:
			pagina web index.html       usó jquery-2.1.3.js y keypress.js y bootstrap    recibe un .csv y permite visualizar los comentarios.. con las frases destacadas con colores si es directiva o de otro tipo (mas tipos fueron agregados posteriormente). uso de hotkeys fue importante para facilitar el proceso un poco tedioso de navegar por varios cientos de comentarios y marcar o desmarcar con colores algunas frases.   Pagina creada para visualizar y para corregir los tipos/clases de las frases. Exporta datos en forma de texto , inicialmente procesados a mano por un script... pero automatizado posteriormente. La solucion es Web porque es mas compatible que otras soluciones.

			hablar algo de script que procesa output de la web 3_process_web_output.py

			Hablar de los intentos realizados para mejorar la visualización de las frases destacadas en la pagina web. Se intentó removiendo las comillas "" y todos los tags html (usando regexp). Luego se sacaron los tags <p> <li>


	ML, WEKA:
		hablar de lo que hice en API de Weka en Java.Eclipse y su potencial, que se puede hacer con eso.
		string to word vector - intentos cambiando parametros como min-term-freq, otros tokenizers
		analisis de nodos del arbol
		seleccion de atributos y seleccion manual de atributos
		comparacion de varios algoritmos, ver libro1.xlsx y resultados del Weka.Experimenter
		intento reemplazando camelcase-words, filepaths, class paths como Java.Utils
		intento con stopwords
			stopwords elegidas, en base a un archivo oficial de google
		intento con stemming
			lovins, iterated lovins

	software final:
			explicar extraccion de comentarios de varios archivos en varias carpetas.
			... etc (pendiente)...
	
	Destacacion de comentarios en Sublime Text hecho por mi. (es un ejemplo que se pueden ver de esa forma las directivas) (ver carpeta E, alli hay imagenes de ejemplo de lo que hice)

	@tag.usage.general usado en eclipse con eMoose destaca y avisa de las directivas

	conversion a .arff (non-sparse y sparse)

	Comparar Keywords de Monperrus con nodos superiores del arbol J48.


Trabajo Futuro:
		Usar Machine Learning para obtener mayor conocimiento (insight) de la estructura de los datos del problema, es decir, mayor conocimiento sobre los comentarios y directivas. Este trabajo abarcó el uso de ML para predecir y clasificar comentarios, pero no abarca la obtención de mayor entendimiento sobre los datos de entrada (los comentarios).



Bibliografia:
	Incluir en bibliografia: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf   apéndice C – (justifica el porque de usar LibLinear en Weka para datos tipo texto.

	Incluir , para el uso practico de liblinear: (Appendix N) http://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf







Comment Classifier
Comments Highlighter (CHI o CHi)
Jah Api Highlighter (JAH)
Java Api Highlighter (JAH)















id,id_sub,is_directive
10011,3,3
10016,4,1
10018,3,3
10025,2,2
10025,3,2
10025,5,3
10042,2,1
10042,5,2
10042,6,2
10045,2,1
10045,3,2
10175,1,0
10178,3,3
10214,2,0
10215,2,0
10216,5,1
10238,2,2
10238,4,3
10239,2,2
10239,4,3
10246,3,1
10247,2,0
10247,3,2
10248,3,1
10255,2,1
10255,4,1
10255,5,1
10255,6,0
10256,3,3
10275,2,0
7766,2,2
7766,5,3
7775,3,3
7775,4,3
7783,1,0
7783,2,2
7787,2,2
7794,1,0
7794,2,2
7794,4,0
7829,5,0
7832,1,0
7851,1,0
7852,1,0
7852,3,2
7865,2,3
7867,1,0
7867,7,0
7871,2,2
7871,4,3
7872,2,2
7872,3,0
7872,4,0
7873,2,2
7873,4,3
7873,5,0
7873,6,0
7876,2,2
7891,2,2
7913,2,2
7914,3,2
7914,8,2
8032,3,2
8032,4,2
8058,2,0
8083,2,2
8083,4,2
8084,2,2
8084,4,2
8085,2,2
8085,4,2
8125,2,2
8147,2,2
8234,2,2
8234,3,2
8239,2,3
8243,2,3
8244,1,0
8246,3,3
8265,4,1
8271,2,2
8271,3,3
8274,1,2
8274,2,2
8274,3,3
8279,3,2
8291,2,3
8291,3,3
8296,2,3
8296,3,3
8299,1,2
8299,2,2
8299,3,3
8299,4,3
8301,2,3
8306,4,3
8310,1,2
8310,2,2
8312,2,3
8315,1,2
8315,2,2
8315,3,0
8320,1,2
8320,2,2
8327,1,0
8327,2,0
8327,5,0
8335,2,3
8341,1,0
8344,1,2
8344,2,2
8346,2,1
8346,3,1
8346,4,3
8347,2,1
8347,3,1
8347,4,3
8349,1,2
8350,1,2
8350,2,2
8350,3,3
8356,1,0
8357,1,0
8358,1,2
8358,2,2
8358,3,0
8358,4,3
8358,5,3
8364,1,2
8364,2,2
8370,5,3
8374,1,2
8374,2,2
8374,4,3
8383,4,3
8386,1,2
8386,2,2
8386,3,3
8389,1,0
8391,1,2
8391,2,2
8391,3,3
8403,1,2
8403,2,2
8403,3,0
8410,1,2
8414,4,1
8419,3,0
8422,1,2
8422,2,2
8422,3,2
8422,4,2
8426,3,3
8429,2,3
8432,1,2
8432,2,2
8432,3,0
8442,1,0
8442,2,1
8443,2,3
8443,3,3
8449,2,3
8450,1,0
8450,3,0
8452,1,2
8452,2,2
8452,3,3
8461,2,3
8464,1,2
8464,2,2
8464,3,0
8466,2,3
8469,1,2
8469,2,2
8469,3,0
8478,1,2
8478,2,2
8481,4,1
8489,1,2
8489,2,2
8489,3,3
8489,4,3
8584,2,0
8585,2,1
8585,3,0
8587,2,0
8626,2,1
8634,2,1
8690,2,2
8690,3,2
8690,4,3
8706,3,0
8742,2,1
8744,2,3
8754,2,1
8755,2,1
8772,2,1
8796,3,2
8796,4,3
8797,3,2
8797,4,3
8825,2,1
8829,2,3
8890,2,0
8903,2,2
8945,3,3
8964,1,1
8967,3,3
8999,2,1
9085,2,1
9085,3,1
9085,5,1
9087,2,2
9087,3,1
9087,4,1
9087,5,0
9087,6,1
9087,7,1
9096,1,0
9113,2,1
9113,3,1
9115,2,1
9115,3,1
9117,2,1
9117,3,1
9117,5,1
9119,3,1
9269,2,3
9269,3,3
9371,3,0
9383,2,2
9383,3,0
9383,4,2
9570,3,1
9570,4,1
9570,5,2
9570,6,1
9573,2,1
9573,3,1
9573,4,2
9573,5,1
9578,2,1
9578,3,1
9578,4,2
9578,5,1
9584,2,1
9585,2,1
9588,6,3
9588,7,1
9591,2,3
9634,5,3
9635,4,3
9638,5,3
9640,6,3
9641,5,3
9644,6,3
9646,7,3
9650,7,3
9652,8,3
9653,7,3
9674,3,1
9674,7,3
9675,2,2
9676,2,1
9676,6,3
9680,2,2
9736,3,3
9801,3,3
9809,3,3
9811,2,1
9811,5,2
9828,2,2
9831,2,0
9860,2,0
9862,2,0
9864,2,1
9917,2,0
9955,2,1
9955,4,3
9956,2,1
9957,2,1
9959,2,2
9988,3,3
9990,2,1
9990,3,2
9991,2,1
9991,3,2
9992,2,1
9992,3,2




######## my ignores:
DjangoVideoTutorial/
DjangoOficialTutorial/
*.csv
*.arff
*.pdf

# Windows image file caches
Thumbs.db
ehthumbs.db

# Folder config file
Desktop.ini

# Recycle Bin used on file shares
$RECYCLE.BIN/

# Windows Installer files
*.cab
*.msi
*.msm
*.msp

# Windows shortcuts
*.lnk

# =========================
# Operating System Files
# =========================

# OSX
# =========================

.DS_Store
.AppleDouble
.LSOverride

# Thumbnails
._*

# Files that might appear on external disk
.Spotlight-V100
.Trashes

# Directories potentially created on remote AFP share
.AppleDB
.AppleDesktop
Network Trash Folder
Temporary Items
.apdisk
